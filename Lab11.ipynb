{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "43c90b9d",
      "metadata": {
        "id": "43c90b9d"
      },
      "source": [
        "# MIE424 (2023 Winter) Lab 11 Decision Trees\n",
        "\n",
        "Presented by Bo Tang\n",
        "\n",
        "In this lab, you will dive into the decision tree, specifically focusing on the Classification and Regression Tree (CART) algorithm.\n",
        "\n",
        "CART is a versatile algorithm capable of producing both classification trees (where the target variable is categorical) and regression trees (where the target variable is continuous). It employs a greedy approach, recursively splitting the data into subsets that maximize information gain for classification or minimize mean squared error for regression. However, it s important to note that the CART algorithm is heuristic and does not guarantee the generation of an optimal decision tree.\n",
        "\n",
        "The `scikit-learn` library in Python offers an implementation of CART, to build and evaluate decision trees on real-world data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Packages"
      ],
      "metadata": {
        "id": "nedXpGOxVoF9"
      },
      "id": "nedXpGOxVoF9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3b6df5d",
      "metadata": {
        "id": "b3b6df5d"
      },
      "outputs": [],
      "source": [
        "!pip install mlxtend\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import tree as sktree\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CART Implementation"
      ],
      "metadata": {
        "id": "P4uVE7b6Vqge"
      },
      "id": "P4uVE7b6Vqge"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise, you are tasked with implementing the core logic that decision trees use to decide where to split the data. Specifically, you will calculate the information gain resulting from splitting the data based on different thresholds of a feature.\n",
        "\n",
        "The missing part is in method `_grow_tree`."
      ],
      "metadata": {
        "id": "lD0GDJmFjKHy"
      },
      "id": "lD0GDJmFjKHy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36dd5aed",
      "metadata": {
        "id": "36dd5aed"
      },
      "outputs": [],
      "source": [
        "class CART(object):\n",
        "    def __init__(self, tree=\"cls\", criterion=\"gini\", prune=\"depth\", max_depth=4, min_criterion=0.05):\n",
        "        # init decision tree parameters\n",
        "        self.feature = None # feature used for splitting\n",
        "        self.label = None # label of the node\n",
        "        self.n_samples = None # number of samples at the node\n",
        "        self.depth = 0 # depth of node\n",
        "        self.root = None # root of tree\n",
        "        self.left = None # left subtree\n",
        "        self.right = None # right subtree\n",
        "        self.threshold = None # threshold for splitting\n",
        "        self.gain = None # information gain\n",
        "        # init CART tree parameters\n",
        "        self.tree = tree # type of tree (classification or regression)\n",
        "        self.criterion = criterion # criterion for impurity calculation\n",
        "        self.prune = prune # pruning method\n",
        "        self.max_depth = max_depth # maximum depth of the tree\n",
        "        self.min_criterion = min_criterion # minimum criterion for splitting\n",
        "    def fit(self, features, target):\n",
        "        \"\"\"\n",
        "        A method to train decision decision tree\n",
        "        \"\"\"\n",
        "        # create the root node\n",
        "        self.root = CART()\n",
        "        # build the tree\n",
        "        self.root._grow_tree(features, target, self.criterion)\n",
        "        # prune the tree\n",
        "        self.root._prune(self.prune, self.max_depth, self.min_criterion, self.root.n_samples)\n",
        "\n",
        "    def predict(self, features):\n",
        "        \"\"\"\n",
        "        A method to predict labels for input features\n",
        "        \"\"\"\n",
        "        return np.array([self.root._predict(f) for f in features])\n",
        "\n",
        "    def _grow_tree(self, features, target, criterion = 'gini'):\n",
        "        \"\"\"\n",
        "        A method to grow the tree recursively\n",
        "        \"\"\"\n",
        "        # update the number of samples at the node\n",
        "        self.n_samples = features.shape[0]\n",
        "\n",
        "        # if all targets are the same, set the label of the node\n",
        "        if len(np.unique(target)) == 1:\n",
        "            self.label = target[0]\n",
        "            return\n",
        "        # select the most frequently occuring class to be the label\n",
        "        self.label = max([(c, len(target[target == c])) for c in np.unique(target)], key = lambda x : x[1])[0]\n",
        "\n",
        "        # init best\n",
        "        best_gain = 0.0 # best gain\n",
        "        best_feature = None # best feature for splitting\n",
        "        best_threshold = None # best threshold for splitting\n",
        "        # calculate the current impurity of the node\n",
        "        impurity_node = self._calc_impurity(criterion, target)\n",
        "        # iterate through all features to find the best split\n",
        "        for col in range(features.shape[1]):\n",
        "            feature_level = np.unique(features[:,col])\n",
        "            # calculate thresholds as midpoints between consecutive feature values\n",
        "            thresholds = (feature_level[:-1] + feature_level[1:]) / 2.0\n",
        "            # iterate through all thresholds to find the best split\n",
        "            for threshold in thresholds:\n",
        "                ########################### Fill in ###########################\n",
        "                # split the data and calculate impurity for each subset\n",
        "                # impurity of left\n",
        "                target_l =\n",
        "                impurity_l =\n",
        "                n_l =\n",
        "                # impurity of right\n",
        "                target_r =\n",
        "                impurity_r =\n",
        "                n_r =\n",
        "                # calculate the information gain from this split\n",
        "                impurity_gain =\n",
        "                # update best information gain\n",
        "                if impurity_gain > best_gain:\n",
        "                    best_gain = impurity_gain\n",
        "                    best_feature = col\n",
        "                    best_threshold = threshold\n",
        "        # set the best feature, threshold, and gain for the current node\n",
        "        self.feature = best_feature\n",
        "        self.gain = best_gain\n",
        "        self.threshold = best_threshold\n",
        "        # recursively grow the left and right subtrees\n",
        "        if best_gain > 0:\n",
        "            self._split_tree(features, target, criterion)\n",
        "\n",
        "    def _split_tree(self, features, target, criterion):\n",
        "        \"\"\"\n",
        "        A method to split the data based on the best feature and threshold\n",
        "        \"\"\"\n",
        "        # split the data to left based on the feature and threshold\n",
        "        features_l = features[features[:, self.feature] <= self.threshold]\n",
        "        target_l = target[features[:, self.feature] <= self.threshold]\n",
        "        # build the left subtree\n",
        "        self.left = CART() # create the left node\n",
        "        self.left.depth = self.depth + 1 # increment depth for the child node\n",
        "        self.left._grow_tree(features_l, target_l, criterion) # grow the left subtree\n",
        "        # split the data to right based on the feature and threshold\n",
        "        features_r = features[features[:, self.feature] > self.threshold]\n",
        "        target_r = target[features[:, self.feature] > self.threshold]\n",
        "        # build the right subtree\n",
        "        self.right = CART() # create the right node\n",
        "        self.right.depth = self.depth + 1 # increment depth for the child node\n",
        "        self.right._grow_tree(features_r, target_r, criterion) # grow the right subtree\n",
        "\n",
        "    def _calc_impurity(self, criterion, target):\n",
        "        \"\"\"\n",
        "        A method to calculate the impurity of a node using Gini or entropy\n",
        "        \"\"\"\n",
        "        # Gini impurity calculation\n",
        "        if criterion == \"gini\":\n",
        "            return 1.0 - sum([(float(len(target[target == c])) / float(target.shape[0])) ** 2.0 for c in np.unique(target)])\n",
        "        # entropy calculation\n",
        "        else:\n",
        "            entropy = 0.0\n",
        "            for c in np.unique(target):\n",
        "                p = float(len(target[target == c])) / target.shape[0]\n",
        "                if p > 0.0:\n",
        "                    entropy -= p * np.log2(p)\n",
        "            return entropy\n",
        "\n",
        "    def _prune(self, method, max_depth, min_criterion, n_samples):\n",
        "        \"\"\"\n",
        "        A method to prune the tree by impurity or depth\n",
        "        \"\"\"\n",
        "        # it is a leaf node, no prune\n",
        "        if self.feature is None:\n",
        "            return\n",
        "        # prune the left and right subtrees recursively\n",
        "        self.left._prune(method, max_depth, min_criterion, n_samples)\n",
        "        self.right._prune(method, max_depth, min_criterion, n_samples)\n",
        "        # decide whether to prune this node based on the pruning method\n",
        "        if method == \"impurity\" and self.left.feature is None and self.right.feature is None:\n",
        "            if (self.gain * float(self.n_samples) / n_samples) < min_criterion:\n",
        "                 # convert into a leaf node\n",
        "                self.left = None\n",
        "                self.right = None\n",
        "                self.feature = None\n",
        "        elif method == \"depth\" and self.depth >= max_depth:\n",
        "            # convert into a leaf node\n",
        "            self.left = None\n",
        "            self.right = None\n",
        "            self.feature = None\n",
        "\n",
        "    def _predict(self, d):\n",
        "        \"\"\"\n",
        "        # A medthod to predict the label for a given instance\n",
        "        \"\"\"\n",
        "        # this is not a leaf node, traverse further down the tree\n",
        "        if self.feature != None:\n",
        "            # traverse to left subtree\n",
        "            if d[self.feature] <= self.threshold:\n",
        "                return self.left._predict(d)\n",
        "            # traverse to right subtree\n",
        "            else:\n",
        "                return self.right._predict(d)\n",
        "        # this is a leaf node, return the label\n",
        "        else:\n",
        "            return self.label\n",
        "\n",
        "    def print_tree(self):\n",
        "        \"\"\"\n",
        "        A method to print the tree structure\n",
        "        \"\"\"\n",
        "        self.root._show_tree(0, \" \")\n",
        "\n",
        "    def _show_tree(self, depth, cond):\n",
        "        \"\"\"\n",
        "        A helper method to print the tree structure\n",
        "        \"\"\"\n",
        "        base = \"    \" * depth + cond\n",
        "        if self.feature != None:\n",
        "            print(base + \"if X[\" + str(self.feature) + \"] <= \" + str(self.threshold))\n",
        "            self.left._show_tree(depth+1, \"then \")\n",
        "            self.right._show_tree(depth+1, \"else \")\n",
        "        else:\n",
        "            print(base + \"{value: \" + str(self.label) + \", samples: \" + str(self.n_samples) + \"}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n",
        "\n",
        "Use the Iris dataset, a simple dataset, for the experiment"
      ],
      "metadata": {
        "id": "NX7q4E5djcvL"
      },
      "id": "NX7q4E5djcvL"
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ],
      "metadata": {
        "id": "efspEvVQjnRN"
      },
      "id": "efspEvVQjnRN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluation"
      ],
      "metadata": {
        "id": "JMdrU9Zaj8SY"
      },
      "id": "JMdrU9Zaj8SY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can compare the performance with the CART from the `scikit-learn`.\n",
        "\n",
        "Both are configured to use the \"entropy\" criterion."
      ],
      "metadata": {
        "id": "5y0dKZw_ksNz"
      },
      "id": "5y0dKZw_ksNz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcca8d17",
      "metadata": {
        "id": "bcca8d17"
      },
      "outputs": [],
      "source": [
        "# implemented CART\n",
        "cls = CART(tree=\"cls\", criterion=\"entropy\", prune=\"depth\", max_depth=3)\n",
        "cls.fit(X_train, y_train)\n",
        "cls.print_tree()\n",
        "pred = cls.predict(X_test)\n",
        "print(\"This Classification Tree Prediction Accuracy:    {}\".format(sum(pred == y_test) / len(pred)))\n",
        "\n",
        "# sklearn decision tree\n",
        "clf = sktree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n",
        "clf = clf.fit(X_train, y_train)\n",
        "sk_pred = clf.predict(X_test)\n",
        "print(\"Sklearn Library Tree Prediction Accuracy:        {}\".format(sum(sk_pred == y_test) / len(pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "035a3395",
      "metadata": {
        "id": "035a3395"
      },
      "source": [
        "Using the Gini criterion instead:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15615b33",
      "metadata": {
        "id": "15615b33"
      },
      "outputs": [],
      "source": [
        "# custom CART\n",
        "cls = CART(tree=\"cls\", criterion=\"gini\", prune=\"depth\", max_depth=3, min_criterion=0)\n",
        "cls.fit(X_train, y_train)\n",
        "cls.print_tree()\n",
        "pred = cls.predict(X_test)\n",
        "print(\"This Classification Tree Prediction Accuracy:    {}\".format(sum(pred == y_test) / len(pred)))\n",
        "\n",
        "# sklearn decision tree\n",
        "clf = sktree.DecisionTreeClassifier(criterion=\"gini\", max_depth=3)\n",
        "clf.fit(X_train, y_train)\n",
        "sk_pred = clf.predict(X_test)\n",
        "print(\"Sklearn Library Tree Prediction Accuracy:        {}\".format(sum(sk_pred == y_test) / len(pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d62f30d",
      "metadata": {
        "id": "2d62f30d"
      },
      "source": [
        "## Visualizing the decision boundary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48f3cc6c",
      "metadata": {
        "id": "48f3cc6c"
      },
      "outputs": [],
      "source": [
        "# reduce feature set to 2 dimensions for visualizations\n",
        "X_new = X[:,[2,3]] # reduce feature set to 2 dimensions for visualizations\n",
        "# train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_new, y, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# custom CART\n",
        "cls = CART(tree=\"cls\", criterion=\"entropy\", prune=\"depth\", max_depth=3)\n",
        "cls.fit(X_train, y_train)\n",
        "cls.print_tree()\n",
        "pred = cls.predict(X_test)\n",
        "print(\"This Classification Tree Prediction Accuracy:    {}\".format(sum(pred == y_test) / len(pred)))"
      ],
      "metadata": {
        "id": "qSlAJojKmXam"
      },
      "id": "qSlAJojKmXam",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b21c9bb",
      "metadata": {
        "id": "4b21c9bb"
      },
      "outputs": [],
      "source": [
        "# draw plot\n",
        "plot_decision_regions(X_train, y_train, cls)\n",
        "plt.xlabel('petal length [cm]')\n",
        "plt.ylabel('petal width [cm]')\n",
        "plt.legend(loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.title(\"Custom Decision Tree Decision Boundary with Entropy\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# custom CART with Gini\n",
        "cls = CART(tree=\"cls\", criterion=\"gini\", prune=\"depth\", max_depth=3)\n",
        "cls.fit(X_train, y_train)\n",
        "cls.print_tree()\n",
        "pred = cls.predict(X_test)\n",
        "print(\"This Classification Tree Prediction Accuracy:    {}\".format(sum(pred == y_test) / len(pred)))"
      ],
      "metadata": {
        "id": "wjBLUiG60ASo"
      },
      "id": "wjBLUiG60ASo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# draw plot\n",
        "plot_decision_regions(X_train, y_train, cls)\n",
        "plt.xlabel('petal length [cm]')\n",
        "plt.ylabel('petal width [cm]')\n",
        "plt.legend(loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.title(\"Custom Decision Tree Decision Boundary with Gini\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mjzc_Q8v0AZv"
      },
      "id": "mjzc_Q8v0AZv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Bonus) Optimal Decision Tree\n",
        "\n",
        "Algorithms like CART and ID3 use greedy strategies to build decision trees by making locally optimal choices at each node. However, these choices might not lead to the globally optimal tree.\n",
        "\n",
        "A notable advancement in this area is the Optimal Classification Tree (OCT) algorithm proposed by [Bertsimas and Dunn]((https://link.springer.com/article/10.1007/s10994-017-5633-9). OCT approaches decision tree construction as a Mixed Integer Programming (MIP) problem, allowing for the direct optimization of the tree structure with respect to a global objective function.\n",
        "\n",
        "The implementation of this algorithm is available in a [Github Repo](https://github.com/LucasBoTang/Optimal_Classification_Trees). This Repo also contains other optimal decision tree models."
      ],
      "metadata": {
        "id": "qAS5TfCBnrcY"
      },
      "id": "qAS5TfCBnrcY"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gurobipy\n",
        "from collections import namedtuple\n",
        "from scipy import stats\n",
        "import gurobipy as gp\n",
        "from gurobipy import GRB"
      ],
      "metadata": {
        "id": "k5hgosEgoDAC"
      },
      "id": "k5hgosEgoDAC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class optimalDecisionTreeClassifier:\n",
        "    \"\"\"\n",
        "    optimal classification tree\n",
        "    \"\"\"\n",
        "    def __init__(self, max_depth=3, min_samples_split=2, alpha=0, warmstart=True, timelimit=600, output=True):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.alpha = alpha\n",
        "        self.warmstart = warmstart\n",
        "        self.timelimit = timelimit\n",
        "        self.output = output\n",
        "        self.trained = False\n",
        "        self.optgap = None\n",
        "        # node index\n",
        "        self.n_index = [i+1 for i in range(2 ** (self.max_depth + 1) - 1)]\n",
        "        self.b_index = self.n_index[:-2**self.max_depth] # branch nodes\n",
        "        self.l_index = self.n_index[-2**self.max_depth:] # leaf nodes\n",
        "\n",
        "    def fit(self, x, y):\n",
        "        \"\"\"\n",
        "        fit training data\n",
        "        \"\"\"\n",
        "        # data size\n",
        "        self.n, self.p = x.shape\n",
        "        if self.output:\n",
        "            print('Training data include {} instances, {} features.'.format(self.n,self.p))\n",
        "\n",
        "        # labels\n",
        "        self.labels = np.unique(y)\n",
        "\n",
        "        # scale data\n",
        "        self.scales = np.max(x, axis=0)\n",
        "        self.scales[self.scales == 0] = 1\n",
        "\n",
        "        # solve MIP\n",
        "        m, a, b, c, d, l = self._buildMIP(x/self.scales, y)\n",
        "        if self.warmstart:\n",
        "            self._setStart(x, y, a, c, d, l)\n",
        "        m.optimize()\n",
        "        self.optgap = m.MIPGap\n",
        "\n",
        "        # get parameters\n",
        "        self._a = {ind:a[ind].x for ind in a}\n",
        "        self._b = {ind:b[ind].x for ind in b}\n",
        "        self._c = {ind:c[ind].x for ind in c}\n",
        "        self._d = {ind:d[ind].x for ind in d}\n",
        "\n",
        "        self.trained = True\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"\n",
        "        model prediction\n",
        "        \"\"\"\n",
        "        if not self.trained:\n",
        "            raise AssertionError('This optimalDecisionTreeClassifier instance is not fitted yet.')\n",
        "\n",
        "        # leaf label\n",
        "        labelmap = {}\n",
        "        for t in self.l_index:\n",
        "            for k in self.labels:\n",
        "                if self._c[k,t] >= 1e-2:\n",
        "                    labelmap[t] = k\n",
        "        y_pred = []\n",
        "        for xi in x/self.scales:\n",
        "            t = 1\n",
        "            while t not in self.l_index:\n",
        "                right = (sum([self._a[j,t] * xi[j] for j in range(self.p)]) + 1e-9 >= self._b[t])\n",
        "                if right:\n",
        "                    t = 2 * t + 1\n",
        "                else:\n",
        "                    t = 2 * t\n",
        "            # label\n",
        "            y_pred.append(labelmap[t])\n",
        "\n",
        "        return np.array(y_pred)\n",
        "\n",
        "    def _buildMIP(self, x, y):\n",
        "        \"\"\"\n",
        "        build MIP formulation for Optimal Decision Tree\n",
        "        \"\"\"\n",
        "        # create a model\n",
        "        m = gp.Model('m')\n",
        "        # output\n",
        "        m.Params.outputFlag = self.output\n",
        "        m.Params.LogToConsole = self.output\n",
        "        # time limit\n",
        "        m.Params.timelimit = self.timelimit\n",
        "        # parallel\n",
        "        m.params.threads = 0\n",
        "        # model sense\n",
        "        m.modelSense = GRB.MINIMIZE\n",
        "        # variables\n",
        "        a = m.addVars(self.p, self.b_index, vtype=GRB.BINARY, name='a') # splitting feature\n",
        "        b = m.addVars(self.b_index, vtype=GRB.CONTINUOUS, name='b') # splitting threshold\n",
        "        c = m.addVars(self.labels, self.l_index, vtype=GRB.BINARY, name='c') # node prediction\n",
        "        d = m.addVars(self.b_index, vtype=GRB.BINARY, name='d') # splitting option\n",
        "        z = m.addVars(self.n, self.l_index, vtype=GRB.BINARY, name='z') # leaf node assignment\n",
        "        l = m.addVars(self.l_index, vtype=GRB.BINARY, name='l') # leaf node activation\n",
        "        L = m.addVars(self.l_index, vtype=GRB.CONTINUOUS, name='L') # leaf node misclassified\n",
        "        M = m.addVars(self.labels, self.l_index, vtype=GRB.CONTINUOUS, name='M') # leaf node samples with label\n",
        "        N = m.addVars(self.l_index, vtype=GRB.CONTINUOUS, name='N') # leaf node samples\n",
        "        # calculate baseline accuracy\n",
        "        baseline = self._calBaseline(y)\n",
        "        # calculate minimum distance\n",
        "        min_dis = self._calMinDist(x)\n",
        "        # objective function\n",
        "        obj = L.sum() / baseline + self.alpha * d.sum()\n",
        "        m.setObjective(obj)\n",
        "        # constraints\n",
        "        # (20)\n",
        "        m.addConstrs(L[t] >= N[t] - M[k,t] - self.n * (1 - c[k,t]) for t in self.l_index for k in self.labels)\n",
        "        # (21)\n",
        "        m.addConstrs(L[t] <= N[t] - M[k,t] + self.n * c[k,t] for t in self.l_index for k in self.labels)\n",
        "        # (17)\n",
        "        m.addConstrs(gp.quicksum((y[i] == k) * z[i,t] for i in range(self.n)) == M[k,t]\n",
        "                                 for t in self.l_index for k in self.labels)\n",
        "        # (16)\n",
        "        m.addConstrs(z.sum('*', t) == N[t] for t in self.l_index)\n",
        "        # (18)\n",
        "        m.addConstrs(c.sum('*', t) == l[t] for t in self.l_index)\n",
        "        # (13) and (14)\n",
        "        for t in self.l_index:\n",
        "            left = (t % 2 == 0)\n",
        "            ta = t // 2\n",
        "            while ta != 0:\n",
        "                if left:\n",
        "                    m.addConstrs(gp.quicksum(a[j,ta] * (x[i,j] + min_dis[j]) for j in range(self.p))\n",
        "                                 +\n",
        "                                 (1 + np.max(min_dis)) * (1 - d[ta])\n",
        "                                 <=\n",
        "                                 b[ta] + (1 + np.max(min_dis)) * (1 - z[i,t])\n",
        "                                 for i in range(self.n))\n",
        "                else:\n",
        "                    m.addConstrs(gp.quicksum(a[j,ta] * x[i,j] for j in range(self.p))\n",
        "                                 >=\n",
        "                                 b[ta] - (1 - z[i,t])\n",
        "                                 for i in range(self.n))\n",
        "                left = (ta % 2 == 0)\n",
        "                ta //= 2\n",
        "        # (8)\n",
        "        m.addConstrs(z.sum(i, '*') == 1 for i in range(self.n))\n",
        "        # (6)\n",
        "        m.addConstrs(z[i,t] <= l[t] for t in self.l_index for i in range(self.n))\n",
        "        # (7)\n",
        "        m.addConstrs(z.sum('*', t) >= self.min_samples_split * l[t] for t in self.l_index)\n",
        "        # (2)\n",
        "        m.addConstrs(a.sum('*', t) == d[t] for t in self.b_index)\n",
        "        # (3)\n",
        "        m.addConstrs(b[t] <= d[t] for t in self.b_index)\n",
        "        # (5)\n",
        "        m.addConstrs(d[t] <= d[t//2] for t in self.b_index if t != 1)\n",
        "\n",
        "        return m, a, b, c, d, l\n",
        "\n",
        "    @staticmethod\n",
        "    def _calBaseline(y):\n",
        "        \"\"\"\n",
        "        obtain baseline accuracy by simply predicting the most popular class\n",
        "        \"\"\"\n",
        "        mode = stats.mode(y)[0]\n",
        "        return np.sum(y == mode)\n",
        "\n",
        "    @staticmethod\n",
        "    def _calMinDist(x):\n",
        "        \"\"\"\n",
        "        get the smallest non-zero distance of features\n",
        "        \"\"\"\n",
        "        min_dis = []\n",
        "        for j in range(x.shape[1]):\n",
        "            xj = x[:,j]\n",
        "            # drop duplicates\n",
        "            xj = np.unique(xj)\n",
        "            # sort\n",
        "            xj = np.sort(xj)[::-1]\n",
        "            # distance\n",
        "            dis = [1]\n",
        "            for i in range(len(xj)-1):\n",
        "                dis.append(xj[i] - xj[i+1])\n",
        "            # min distance\n",
        "            min_dis.append(np.min(dis) if np.min(dis) else 1)\n",
        "        return min_dis\n",
        "\n",
        "    def _setStart(self, x, y, a, c, d, l):\n",
        "        \"\"\"\n",
        "        set warm start from CART\n",
        "        \"\"\"\n",
        "        # train with CART\n",
        "        if self.min_samples_split > 1:\n",
        "            clf = sktree.DecisionTreeClassifier(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
        "        else:\n",
        "            clf = sktree.DecisionTreeClassifier(max_depth=self.max_depth)\n",
        "        clf.fit(x, y)\n",
        "\n",
        "        # get splitting rules\n",
        "        rules = self._getRules(clf)\n",
        "\n",
        "        # fix branch node\n",
        "        for t in self.b_index:\n",
        "            # not split\n",
        "            if rules[t].feat is None or rules[t].feat == sktree._tree.TREE_UNDEFINED:\n",
        "                d[t].start = 0\n",
        "                for f in range(self.p):\n",
        "                    a[f,t].start = 0\n",
        "            # split\n",
        "            else:\n",
        "                d[t].start = 1\n",
        "                for f in range(self.p):\n",
        "                    if f == int(rules[t].feat):\n",
        "                        a[f,t].start = 1\n",
        "                    else:\n",
        "                        a[f,t].start = 0\n",
        "\n",
        "        # fix leaf nodes\n",
        "        for t in self.l_index:\n",
        "            # terminate early\n",
        "            if rules[t].value is None:\n",
        "                l[t].start = int(t % 2)\n",
        "                # flows go to right\n",
        "                if t % 2:\n",
        "                    t_leaf = t\n",
        "                    while rules[t].value is None:\n",
        "                        t //= 2\n",
        "                    for k in self.labels:\n",
        "                        if k == np.argmax(rules[t].value):\n",
        "                            c[k, t_leaf].start = 1\n",
        "                        else:\n",
        "                            c[k, t_leaf].start = 0\n",
        "                # nothing in left\n",
        "                else:\n",
        "                    for k in self.labels:\n",
        "                        c[k, t].start = 0\n",
        "            # terminate at leaf node\n",
        "            else:\n",
        "                l[t].start = 1\n",
        "                for k in self.labels:\n",
        "                    if k == np.argmax(rules[t].value):\n",
        "                        c[k, t].start = 1\n",
        "                    else:\n",
        "                        c[k, t].start = 0\n",
        "\n",
        "    def _getRules(self, clf):\n",
        "        \"\"\"\n",
        "        get splitting rules\n",
        "        \"\"\"\n",
        "        # node index map\n",
        "        node_map = {1:0}\n",
        "        for t in self.b_index:\n",
        "            # terminal\n",
        "            node_map[2*t] = -1\n",
        "            node_map[2*t+1] = -1\n",
        "            # left\n",
        "            l = clf.tree_.children_left[node_map[t]]\n",
        "            node_map[2*t] = l\n",
        "            # right\n",
        "            r = clf.tree_.children_right[node_map[t]]\n",
        "            node_map[2*t+1] = r\n",
        "\n",
        "        # rules\n",
        "        rule = namedtuple('Rules', ('feat', 'threshold', 'value'))\n",
        "        rules = {}\n",
        "        # branch nodes\n",
        "        for t in self.b_index:\n",
        "            i = node_map[t]\n",
        "            if i == -1:\n",
        "                r = rule(None, None, None)\n",
        "            else:\n",
        "                r = rule(clf.tree_.feature[i], clf.tree_.threshold[i], clf.tree_.value[i,0])\n",
        "            rules[t] = r\n",
        "        # leaf nodes\n",
        "        for t in self.l_index:\n",
        "            i = node_map[t]\n",
        "            if i == -1:\n",
        "                r = rule(None, None, None)\n",
        "            else:\n",
        "                r = rule(None, None, clf.tree_.value[i,0])\n",
        "            rules[t] = r\n",
        "        return rules"
      ],
      "metadata": {
        "id": "n5hxtl0VnwZo"
      },
      "id": "n5hxtl0VnwZo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimal decision tree\n",
        "oct = optimalDecisionTreeClassifier(max_depth=3)\n",
        "oct.fit(X_train, y_train)\n",
        "oct_pred = oct.predict(X_test)\n",
        "print(\"Optimal Decision Tree Prediction Accuracy:       {}\".format(sum(oct_pred == y_test) / len(pred)))"
      ],
      "metadata": {
        "id": "j4DwdM8zops4"
      },
      "id": "j4DwdM8zops4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fPp0oY_GpSVJ"
      },
      "id": "fPp0oY_GpSVJ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}